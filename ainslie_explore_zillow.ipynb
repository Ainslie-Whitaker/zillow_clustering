{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1a55246",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (wrangle_zillow.py, line 538)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/opt/homebrew/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3437\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-9edc1612fec4>\"\u001b[0;36m, line \u001b[0;32m22\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    import wrangle_zillow\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/Users/loriainslie/codeup-data-science/zillow_clustering/wrangle_zillow.py\"\u001b[0;36m, line \u001b[0;32m538\u001b[0m\n\u001b[0;31m    return pd.DataFrame(data)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.linear_model import LinearRegression, LassoLars, TweedieRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "import graphviz\n",
    "from graphviz import Graph\n",
    "\n",
    "import env\n",
    "import wrangle_zillow\n",
    "import os\n",
    "\n",
    "# turn off pink boxes for demo\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0594c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change display settings to show all columns\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05f9f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a function to pull in zillow data\n",
    "df = wrangle_zillow.wrangle_zillow()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d818d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a function to split data for exploring and modeling\n",
    "train, validate, test = wrangle_zillow.split_data(df)\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d00f2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baa6729",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4247c4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5d286b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check bathroom value counts\n",
    "train.bathrooms.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caf0d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check bedroom value counts\n",
    "train.bedrooms.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dce69ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check binned area value counts\n",
    "train.area.value_counts(bins=10).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05ecf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check binned logerror value counts\n",
    "train.logerror.value_counts(bins=10).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab7fb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check binned logerror value counts\n",
    "train.counties.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7858b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in train.columns:\n",
    "#     plt.figure(figsize=(4,2))\n",
    "#     plt.hist(train[col])\n",
    "#     plt.title(col)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bbe359",
   "metadata": {},
   "source": [
    "### Takeaways:\n",
    "\n",
    "#### Many of the features seem to be skewed to the right\n",
    "\n",
    "#### Bathrooms, bedrooms, area, and age have the highest correlation with logerror\n",
    "\n",
    "#### There are almost twice as many properties in LA County than Orange & Ventura county"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53f9552",
   "metadata": {},
   "source": [
    "## Is there difference in mean logerror for each of the counties?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b76ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e39310b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check mean logerror for each county\n",
    "train.groupby('counties').logerror.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404028e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code is not producing the same result as groupby likely due to rounding\n",
    "train[train.counties == 'los_angeles'].logerror.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1711eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code is not producing the same result as groupby likely due to rounding\n",
    "train[train.counties == 'orange'].logerror.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b3e5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code is not producing the same result as groupby likely due to rounding\n",
    "train[train.counties == 'ventura'].logerror.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de53c4d",
   "metadata": {},
   "source": [
    "**My project partner explored this in greater depth with statistical testing and determined there was a significant difference in logerror by county**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f297c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .describe with object columns.\n",
    "obj_cols = train.columns[[train[col].dtype == 'O' for col in train.columns]]\n",
    "for col in obj_cols:\n",
    "    print(train[col].value_counts())\n",
    "    print(train[col].value_counts(normalize=True, dropna=False))\n",
    "    print('----------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d06e04",
   "metadata": {},
   "source": [
    "## What does absolute logerror look like from county to county?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1356b1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the range for logerror\n",
    "train.logerror.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b4f888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column that bins each value of logerror into max, min, or med absolute error\n",
    "train['log_e'] = pd.cut(train.logerror, bins=[-5,-1,-.03,.03,1,5], ordered=False, labels=['max','med','min','med','max'])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3c3cda",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot the data to see which areas have the most logerror\n",
    "sns.relplot(data=train, x='latitude', y='longitude', hue='log_e', hue_order=['max', 'med','min'], height=10, palette='rocket')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac31dc01",
   "metadata": {},
   "source": [
    "### As seen towards the center of the graph, Los Angeles County does have a higher proportion of med - max logerror which could mean the model has a harder time predicting home values from this location or could just be due to the larger number of properties sold in this area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1cfa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use crosstab to visualize the number of each category per county\n",
    "pd.crosstab(train.counties, train.log_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b7eb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data to see if any county has a higher percentage of max logerrors\n",
    "x, y, hue = 'counties', 'proportion', 'log_e'\n",
    "hue_order = ['max', 'med', 'min']\n",
    "\n",
    "(train[hue]\n",
    " .groupby(train[x])\n",
    " .value_counts(normalize=True)\n",
    " .rename(y)\n",
    " .reset_index()\n",
    " .pipe((sns.barplot, \"data\"), x=x, y=y, hue=hue))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb280a0",
   "metadata": {},
   "source": [
    "### We can barely see on the graph but it looks like the model produces a slightly higher percentage of max errors for Orange County but a higher percentage of medium error for Los Angeles County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6ebbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop log_e to prep for clustering\n",
    "train = train.drop(columns='log_e')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd993a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a function to create X and y datasets for train, validate, and test\n",
    "X_train, y_train, X_validate, y_validate, X_test, y_test = wrangle_zillow.prep_zillow_for_model(train, validate, test)\n",
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6555e084",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942bb35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the features to use\n",
    "X = X_train[['age', 'taxvalue']]\n",
    "X2 = X_validate[['age', 'taxvalue']]\n",
    "X3 = X_test[['age', 'taxvalue']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ec2ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize distribution\n",
    "X.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aa10de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize data to see if there are any obvious clusters\n",
    "sns.relplot(x = 'age', y ='taxvalue', data = X, height=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1918d8",
   "metadata": {},
   "source": [
    "**There are no obvious clusters so I will use the elbow method to see if I can find a good value for k**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d1ead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use elbow method to see what might be a good value for k\n",
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    pd.Series({k: KMeans(k).fit(X).inertia_ for k in range(2, 12)}).plot(marker='x')\n",
    "    plt.xticks(range(2, 12))\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('inertia')\n",
    "    plt.title('Change in inertia as k increases')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7116cb",
   "metadata": {},
   "source": [
    "### Based on this visualization, I will start by using a k of 5 since the slope starts tapering off after that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc72070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use KMeans to create cluster\n",
    "\n",
    "# define the thing\n",
    "kmeans = KMeans(n_clusters=5, random_state = 369)\n",
    "\n",
    "# fit the thing\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Use the thing to predict\n",
    "kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d5fdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column with the predicted cluster in the original X_dataframes\n",
    "X_train['agetax_cluster'] = kmeans.predict(X)\n",
    "X_validate['agetax_cluster'] = kmeans.predict(X2)\n",
    "X_test['agetax_cluster'] = kmeans.predict(X3)\n",
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9247b7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validate.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008b1688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of cluster centers\n",
    "centroids = pd.DataFrame(kmeans.cluster_centers_, columns=X.columns)\n",
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8e0633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize clustering results\n",
    "sns.relplot(x = 'age', y ='taxvalue', data = X_train, hue = 'agetax_cluster')\n",
    "\n",
    "centroids.plot.scatter(x='age', y='taxvalue', c='black', marker='x', s=1000, ax=plt.gca(), label='centroid', figsize=(10, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec6bf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate X and y train dataframes to plot data\n",
    "Xy_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# visualize distribution of logerror by cluster\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.boxplot(\n",
    "    x='agetax_cluster',\n",
    "    y='logerror',\n",
    "    data=Xy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59705398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use KMeans to create 3 clusters to see if that may be more meaningful\n",
    "# define the thing\n",
    "kmeans = KMeans(n_clusters=3, random_state = 369)\n",
    "\n",
    "# fit the thing\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Use the thing to predict\n",
    "kmeans.predict(X)\n",
    "\n",
    "# create a new column with the predicted cluster in the original X_dataframes\n",
    "X_train['agetax_cluster'] = kmeans.predict(X)\n",
    "X_validate['agetax_cluster'] = kmeans.predict(X2)\n",
    "X_test['agetax_cluster'] = kmeans.predict(X3)\n",
    "\n",
    "# create dataframe of cluster centers\n",
    "centroids = pd.DataFrame(kmeans.cluster_centers_, columns=X.columns)\n",
    "\n",
    "# visualize clustering results\n",
    "sns.relplot(x = 'age', y ='taxvalue', data = X_train, hue = 'agetax_cluster')\n",
    "\n",
    "centroids.plot.scatter(x='age', y='taxvalue', c='black', marker='x', s=1000, ax=plt.gca(), label='centroid', figsize=(10, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c4f186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize distribution of logerror by cluster\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.boxplot(\n",
    "    x='agetax_cluster',\n",
    "    y='logerror',\n",
    "    data=Xy_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908a55bf",
   "metadata": {},
   "source": [
    "### Although the visual does not make it appear that there is much of a difference in logerror for each cluster, I am going to apply a statistical test to confirm this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce53ea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51b7a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate X_train and y_train so I can check variance of logerror by cluster\n",
    "X_y = pd.concat([X_train, y_train], axis=1)\n",
    "X_y.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b728a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_y.logerror.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2470eda2",
   "metadata": {},
   "source": [
    "**The null hypothesis for the levene test is that there is equal variance in logerror for the clusters**\n",
    "\n",
    "**The alternate hypothesis for the levene test is that there is unequal variance in logerror for the clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89c81a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set alpha\n",
    "alpha = 0.05\n",
    "\n",
    "# use levene test to check variance of each cluster\n",
    "stat, pvalue = stats.levene(\n",
    "                            X_y[X_y.agetax_cluster == 0].logerror,\n",
    "                            X_y[X_y.agetax_cluster == 1].logerror,\n",
    "                            X_y[X_y.agetax_cluster == 2].logerror)\n",
    "\n",
    "print(f'{stat}, {pvalue}')\n",
    "if pvalue > alpha:\n",
    "    print('We fail to reject the null hypothesis')\n",
    "elif pvalue < alpha:\n",
    "    print('We reject the null hypothesis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1107d230",
   "metadata": {},
   "source": [
    "**Because of unequal variance, I will use the Kruskal-Wallis test to compare the median logerror for each cluster**\n",
    "\n",
    "**The null hypothesis is that there is no significant difference in the median logerror for each of the clusters**\n",
    "\n",
    "**The alternate hypothesis is that there is a significant difference in the median logerror for each of the clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6e5531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set alpha\n",
    "alpha = 0.05\n",
    "\n",
    "# use kruskal-wallis test to compare medians\n",
    "stat, pvalue = stats.kruskal(\n",
    "    X_y[X_y.agetax_cluster == 0].logerror,\n",
    "    X_y[X_y.agetax_cluster == 1].logerror,\n",
    "    X_y[X_y.agetax_cluster == 2].logerror)\n",
    "\n",
    "print(f'{stat}, {pvalue}')\n",
    "if pvalue > alpha:\n",
    "    print('We fail to reject the null hypothesis')\n",
    "elif pvalue < alpha:\n",
    "    print('We reject the null hypothesis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e719a31",
   "metadata": {},
   "source": [
    "### It appears there is a statistically significant difference in logerror for at least two of the clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad81ef0d",
   "metadata": {},
   "source": [
    "**I am going to use selectkbest and recursive feature elimination to see what features might be most relevant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e8d7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters: f_regression stats test, give me 3 features\n",
    "f_selector = SelectKBest(f_regression, k=3)\n",
    "\n",
    "# find the top 3 X's correlated with y\n",
    "f_selector.fit(X_train, y_train)\n",
    "\n",
    "# boolean mask of whether the column was selected or not. \n",
    "feature_mask = f_selector.get_support()\n",
    "\n",
    "# get list of top K features. \n",
    "f_feature = X_train.iloc[:,feature_mask].columns.tolist()\n",
    "f_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af71238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the ML algorithm\n",
    "lm = LinearRegression()\n",
    "\n",
    "# create the rfe object, indicating the ML object (lm) and the number of features I want to end up with. \n",
    "rfe = RFE(lm, n_features_to_select=3)\n",
    "\n",
    "# fit the data using RFE\n",
    "rfe.fit(X_train,y_train)  \n",
    "\n",
    "# get the mask of the columns selected\n",
    "feature_mask = rfe.support_\n",
    "\n",
    "# get list of the column names. \n",
    "rfe_feature = X_train.iloc[:,feature_mask].columns.tolist()\n",
    "rfe_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f44b511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the features to use\n",
    "X = X_train[['bathrooms', 'bedrooms', 'area']]\n",
    "X2 = X_validate[['bathrooms', 'bedrooms', 'area']]\n",
    "X3 = X_test[['bathrooms', 'bedrooms', 'area']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89738e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize distribution for X\n",
    "X.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e39008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize data to see if there are any obvious clusters\n",
    "sns.relplot(x = 'bathrooms', y ='area', hue='bedrooms', data = train, height=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1336d848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use elbow method to see what might be a good value for k\n",
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    pd.Series({k: KMeans(k).fit(X).inertia_ for k in range(2, 12)}).plot(marker='x')\n",
    "    plt.xticks(range(2, 12))\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('inertia')\n",
    "    plt.title('Change in inertia as k increases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd27d558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use KMeans to create 4 clusters\n",
    "\n",
    "# define the thing\n",
    "kmeans = KMeans(n_clusters=4, random_state = 369)\n",
    "\n",
    "# fit the thing\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Use the thing to predict\n",
    "kmeans.predict(X)\n",
    "\n",
    "# create a new column with the predicted cluster in the original X_train\n",
    "X_train['bedbath_area_cluster'] = kmeans.predict(X)\n",
    "X_validate['bedbath_area_cluster'] = kmeans.predict(X2)\n",
    "X_test['bedbath_area_cluster'] = kmeans.predict(X3)\n",
    "\n",
    "# create dataframe of cluster centers\n",
    "centroids = pd.DataFrame(kmeans.cluster_centers_, columns=X.columns)\n",
    "\n",
    "# visualize clustering results\n",
    "sns.relplot(x = 'bathrooms', y ='area', data = X_train, hue = 'bedrooms', col='bedbath_area_cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbedc2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize distribution of logerror by cluster\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.boxplot(\n",
    "    x='bedbath_area_cluster',\n",
    "    y=y_train,\n",
    "    data=X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618f0b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate X_train and y_train so I can check variance of logerror by cluster\n",
    "X_y = pd.concat([X_train, y_train], axis=1)\n",
    "X_y.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60dc4b3",
   "metadata": {},
   "source": [
    "**The null hypothesis for the levene test is that there is equal variance in logerror for the clusters**\n",
    "\n",
    "**The alternate hypothesis for the levene test is that there is unequal variance in logerror for the clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6e71ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set alpha\n",
    "alpha = 0.05\n",
    "\n",
    "# use levene test to check variance of each cluster\n",
    "stat, pvalue = stats.levene(\n",
    "                            X_y[X_y.bedbath_area_cluster == 0].logerror,\n",
    "                            X_y[X_y.bedbath_area_cluster == 1].logerror,\n",
    "                            X_y[X_y.bedbath_area_cluster == 2].logerror,\n",
    "                            X_y[X_y.bedbath_area_cluster == 2].logerror)\n",
    "\n",
    "print(f'{stat}, {pvalue}')\n",
    "if pvalue > alpha:\n",
    "    print('We fail to reject the null hypothesis')\n",
    "elif pvalue < alpha:\n",
    "    print('We reject the null hypothesis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d935da",
   "metadata": {},
   "source": [
    "**Because of unequal variance, I will use the Kruskal-Wallis test to compare the mean logerror for each cluster**\n",
    "\n",
    "**The null hypothesis is that there is no significant difference in the median logerror for each of the clusters**\n",
    "\n",
    "**The alternate hypothesis is that there is a significant difference in the median logerror for each of the clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6343e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set alpha\n",
    "alpha = 0.05\n",
    "\n",
    "# use kruskal-wallis test to compare medians\n",
    "stats.kruskal(\n",
    "    X_y[X_y.bedbath_area_cluster == 0].logerror,\n",
    "    X_y[X_y.bedbath_area_cluster == 1].logerror,\n",
    "    X_y[X_y.bedbath_area_cluster == 2].logerror)\n",
    "\n",
    "print(f'{stat}, {pvalue}')\n",
    "if pvalue > alpha:\n",
    "    print('We fail to reject the null hypothesis')\n",
    "elif pvalue < alpha:\n",
    "    print('We reject the null hypothesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a80375b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use groupby to check logerror stats for each of the clusters\n",
    "X_y.groupby('bedbath_area_cluster').logerror.agg(['min', 'median','mean', 'max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ae39b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51d3594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# give clusters names\n",
    "X_train.agetax_cluster = X_train.agetax_cluster.map({0: \"older_lowtaxvalue\",\n",
    "                                                     1: \"newer_lowtaxvalue\",\n",
    "                                                     2: \"all_ages_hightaxvalue\"})\n",
    "\n",
    "X_validate.agetax_cluster = X_validate.agetax_cluster.map({0: \"older_lowtaxvalue\",\n",
    "                                                           1: \"newer_lowtaxvalue\",\n",
    "                                                           2: \"all_ages_hightaxvalue\"})\n",
    "\n",
    "X_test.agetax_cluster = X_test.agetax_cluster.map({0: \"older_lowtaxvalue\",\n",
    "                                                     1: \"newer_lowtaxvalue\",\n",
    "                                                     2: \"all_ages_hightaxvalue\"})\n",
    "\n",
    "X_train.bedbath_area_cluster = X_train.bedbath_area_cluster.map({0: \"large_3plusbed\",\n",
    "                                                                 1: \"small_2bed\",\n",
    "                                                                 2: \"tiny_1bed\",\n",
    "                                                                 3: \"medium_3bed\"})\n",
    "\n",
    "X_validate.bedbath_area_cluster = X_validate.bedbath_area_cluster.map({0: \"large_3plusbed\",\n",
    "                                                                       1: \"small_2bed\",\n",
    "                                                                       2: \"tiny_1bed\",\n",
    "                                                                       3: \"medium_3bed\"})\n",
    "\n",
    "X_test.bedbath_area_cluster = X_test.bedbath_area_cluster.map({0: \"large_3plusbed\",\n",
    "                                                                 1: \"small_2bed\",\n",
    "                                                                 2: \"tiny_1bed\",\n",
    "                                                                 3: \"medium_3bed\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c005841c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.agetax_cluster.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bab345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode cluster columns\n",
    "X_train_model = pd.get_dummies(X_train[['agetax_cluster','bedbath_area_cluster']])\n",
    "X_validate_model = pd.get_dummies(X_validate[['agetax_cluster','bedbath_area_cluster']])\n",
    "X_test_model = pd.get_dummies(X_test[['agetax_cluster','bedbath_area_cluster']])\n",
    "X_train_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bdb35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00502fd",
   "metadata": {},
   "source": [
    "### First I am going to calculate RMSE for predicting median as the baseline. This will give us something to evaluate our other models against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d58a968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn y for each dataset from a series to a dataframe\n",
    "y_train = pd.DataFrame(y_train)\n",
    "y_validate = pd.DataFrame(y_validate)\n",
    "y_test = pd.DataFrame(y_test)\n",
    "\n",
    "# create a baseline\n",
    "y_train['baseline'] = y_train.logerror.median()\n",
    "y_validate['baseline'] = y_train.logerror.median()\n",
    "y_test['baseline'] = y_train.logerror.median()\n",
    "\n",
    "# 4. RMSE of logerror median\n",
    "rmse_train_baseline = mean_squared_error(y_train.logerror, y_train.baseline)**(1/2)\n",
    "rmse_validate_baseline = mean_squared_error(y_validate.logerror, y_validate.baseline)**(1/2)\n",
    "\n",
    "print(\"RMSE using Median\\nTrain/In-Sample: \", round(rmse_train_baseline, 5), \n",
    "      \"\\nValidate/Out-of-Sample: \", round(rmse_validate_baseline, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882f348e",
   "metadata": {},
   "source": [
    "### In order to beat baseline, any of the models would need to have a RMSE of less than 0.02197 on the train dataset and 0.02068 on the validate dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea34c24",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aead15e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model object\n",
    "lm = LinearRegression(normalize=True)\n",
    "\n",
    "# fit the model to our training data and specify y column \n",
    "lm.fit(X_train_model, y_train.logerror)\n",
    "\n",
    "# predict train & validate\n",
    "y_train['logerror_pred_lm'] = lm.predict(X_train_model)\n",
    "y_validate['logerror_pred_lm'] = lm.predict(X_validate_model)\n",
    "\n",
    "# evaluate train & validate: rmse\n",
    "rmse_train_OLS = mean_squared_error(y_train.logerror, y_train.logerror_pred_lm)**(1/2)\n",
    "rmse_validate_OLS = mean_squared_error(y_validate.logerror, y_validate.logerror_pred_lm)**(1/2)\n",
    "\n",
    "print(\"RMSE for OLS using LinearRegression\\nTraining/In-Sample: \", round(rmse_train_OLS, 5), \n",
    "      \"\\nValidation/Out-of-Sample: \", round(rmse_validate_OLS, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3739161",
   "metadata": {},
   "source": [
    "## LassoLars Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f50cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model object\n",
    "lars = LassoLars(alpha=1)\n",
    "\n",
    "# fit the model to our training data and specify y column \n",
    "lars.fit(X_train_model, y_train.logerror)\n",
    "\n",
    "# predict train & validate\n",
    "y_train['logerror_pred_lars'] = lars.predict(X_train_model)\n",
    "y_validate['logerror_pred_lars'] = lars.predict(X_validate_model)\n",
    "\n",
    "# evaluate train & validate: rmse\n",
    "rmse_train_lars = mean_squared_error(y_train.logerror, y_train.logerror_pred_lars)**(1/2)\n",
    "rmse_validate_lars = mean_squared_error(y_validate.logerror, y_validate.logerror_pred_lars)**(1/2)\n",
    "\n",
    "print(\"RMSE for Lasso + Lars\\nTraining/In-Sample: \", round(rmse_train_lars, 5), \n",
    "      \"\\nValidation/Out-of-Sample: \", round(rmse_validate_lars, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8d9824",
   "metadata": {},
   "source": [
    "**I tried different values of alpha from .01 to 1.5 and received the same results with all of them.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c50239",
   "metadata": {},
   "source": [
    "## Tweedie Regressor (GLM) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be051281",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.logerror.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761d2a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model object\n",
    "glm = TweedieRegressor(power=0, alpha=0.01)\n",
    "\n",
    "# fit the model to our training data and specify y column \n",
    "glm.fit(X_train_model, y_train.logerror)\n",
    "\n",
    "# predict train & validate\n",
    "y_train['logerror_pred_glm'] = glm.predict(X_train_model)\n",
    "y_validate['logerror_pred_glm'] = glm.predict(X_validate_model)\n",
    "\n",
    "# evaluate train & validate: rmse\n",
    "rmse_train_glm = mean_squared_error(y_train.logerror, y_train.logerror_pred_glm)**(1/2)\n",
    "rmse_validate_glm = mean_squared_error(y_validate.logerror, y_validate.logerror_pred_glm)**(1/2)\n",
    "\n",
    "print(\"RMSE for GLM using Tweedie, power=0 & alpha=1\\nTraining/In-Sample: \", round(rmse_train_glm, 5), \n",
    "      \"\\nValidation/Out-of-Sample: \", round(rmse_validate_glm, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b62c895",
   "metadata": {},
   "source": [
    "**I tried different values for power and alpha from .01 to 1 and achieved the best result with a smaller value of alpha**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba69e85a",
   "metadata": {},
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2673760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the polynomial features to get a new set of features\n",
    "pf = PolynomialFeatures(degree=2)\n",
    "\n",
    "# fit and transform X_train_model\n",
    "X_train_degree2 = pf.fit_transform(X_train_model)\n",
    "\n",
    "# transform X_validate_model & X_test_model\n",
    "X_validate_degree2 = pf.transform(X_validate_model)\n",
    "X_test_degree2 = pf.transform(X_test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9fdec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model object\n",
    "lm2 = LinearRegression(normalize=True)\n",
    "\n",
    "# fit the model to our training data and specify y column \n",
    "lm2.fit(X_train_degree2, y_train.logerror)\n",
    "\n",
    "# predict train & validate\n",
    "y_train['logerror_pred_lm2'] = lm2.predict(X_train_degree2)\n",
    "y_validate['logerror_pred_lm2'] = lm2.predict(X_validate_degree2)\n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_train_pr = mean_squared_error(y_train.logerror, y_train.logerror_pred_lm2)**(1/2)\n",
    "rmse_validate_pr = mean_squared_error(y_validate.logerror, y_validate.logerror_pred_lm2)**(1/2)\n",
    "\n",
    "print(\"RMSE for Polynomial Model, degrees=2\\nTraining/In-Sample: \", round(rmse_train_pr, 5), \n",
    "      \"\\nValidation/Out-of-Sample: \", round(rmse_validate_pr, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a8b3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252eddf0",
   "metadata": {},
   "source": [
    "### Let's try a subset of the features to see if we get a better result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536d88af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only bed, bath, area cluster features\n",
    "X_train_bba = X_train_model.iloc[:,3:]\n",
    "X_validate_bba = X_validate_model.iloc[:,3:]\n",
    "X_train_bba.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca25146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model object\n",
    "lm3 = LinearRegression(normalize=True)\n",
    "\n",
    "# fit the model to our training data and specify y column \n",
    "lm3.fit(X_train_bba, y_train.logerror)\n",
    "\n",
    "# predict train & validate\n",
    "y_train['logerror_pred_lm3'] = lm3.predict(X_train_bba)\n",
    "y_validate['logerror_pred_lm3'] = lm3.predict(X_validate_bba)\n",
    "\n",
    "# evaluate train & validate: rmse\n",
    "rmse_train_sub1 = mean_squared_error(y_train.logerror, y_train.logerror_pred_lm3)**(1/2)\n",
    "rmse_validate_sub1 = mean_squared_error(y_validate.logerror, y_validate.logerror_pred_lm3)**(1/2)\n",
    "\n",
    "print(\"RMSE for OLS using LinearRegression\\nTraining/In-Sample: \", round(rmse_train_sub1, 5), \n",
    "      \"\\nValidation/Out-of-Sample: \", round(rmse_validate_sub1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332cf2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only age, tax cluster features\n",
    "X_train_agetax = X_train_model.iloc[:,:3]\n",
    "X_validate_agetax = X_validate_model.iloc[:,:3]\n",
    "X_train_agetax.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75677f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model object\n",
    "lm4 = LinearRegression(normalize=True)\n",
    "\n",
    "# fit the model to our training data and specify y column \n",
    "lm4.fit(X_train_agetax, y_train.logerror)\n",
    "\n",
    "# predict train & validate\n",
    "y_train['logerror_pred_lm4'] = lm4.predict(X_train_agetax)\n",
    "y_validate['logerror_pred_lm4'] = lm4.predict(X_validate_agetax)\n",
    "\n",
    "# evaluate train & validate: rmse\n",
    "rmse_train_sub2 = mean_squared_error(y_train.logerror, y_train.logerror_pred_lm4)**(1/2)\n",
    "rmse_validate_sub2 = mean_squared_error(y_validate.logerror, y_validate.logerror_pred_lm4)**(1/2)\n",
    "\n",
    "print(\"RMSE for OLS using LinearRegression\\nTraining/In-Sample: \", round(rmse_train_sub2, 5), \n",
    "      \"\\nValidation/Out-of-Sample: \", round(rmse_validate_sub2, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ca4ece",
   "metadata": {},
   "source": [
    "### Let's try using latitude and longitude as features for modeling using OLS & Polynomial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156231e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features to use for modeling\n",
    "X_train_latlong = X_train[['latitude','longitude']]\n",
    "X_validate_latlong = X_validate[['latitude','longitude']]\n",
    "X_train_latlong.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e8340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model object\n",
    "lm5 = LinearRegression(normalize=True)\n",
    "\n",
    "# fit the model to our training data and specify y column \n",
    "lm5.fit(X_train_latlong, y_train.logerror)\n",
    "\n",
    "# predict train & validate\n",
    "y_train['logerror_pred_lm5'] = lm5.predict(X_train_latlong)\n",
    "y_validate['logerror_pred_lm5'] = lm5.predict(X_validate_latlong)\n",
    "\n",
    "# evaluate train & validate: rmse\n",
    "rmse_train_sub3 = mean_squared_error(y_train.logerror, y_train.logerror_pred_lm5)**(1/2)\n",
    "rmse_validate_sub3 = mean_squared_error(y_validate.logerror, y_validate.logerror_pred_lm5)**(1/2)\n",
    "\n",
    "print(\"RMSE for OLS using LinearRegression\\nTraining/In-Sample: \", round(rmse_train_sub3, 5), \n",
    "      \"\\nValidation/Out-of-Sample: \", round(rmse_validate_sub3, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8586612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the polynomial features to get a new set of features\n",
    "pf = PolynomialFeatures(degree=2)\n",
    "\n",
    "# fit and transform X_train_model\n",
    "X_train_degree2 = pf.fit_transform(X_train_latlong)\n",
    "\n",
    "# transform X_validate_model & X_test_model\n",
    "X_validate_degree2 = pf.transform(X_validate_latlong)\n",
    "# X_test_degree2 = pf.transform(X_test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4757c2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model object\n",
    "lm6 = LinearRegression(normalize=True)\n",
    "\n",
    "# fit the model to our training data and specify y column \n",
    "lm6.fit(X_train_degree2, y_train.logerror)\n",
    "\n",
    "# predict train & validate\n",
    "y_train['logerror_pred_lm6'] = lm6.predict(X_train_degree2)\n",
    "y_validate['logerror_pred_lm6'] = lm6.predict(X_validate_degree2)\n",
    "\n",
    "# evaluate train & validate: rmse\n",
    "rmse_train_sub3_pr = mean_squared_error(y_train.logerror, y_train.logerror_pred_lm6)**(1/2)\n",
    "rmse_validate_sub3_pr = mean_squared_error(y_validate.logerror, y_validate.logerror_pred_lm6)**(1/2)\n",
    "\n",
    "print(\"RMSE for Polynomial Model, degrees=2\\nTraining/In-Sample: \", round(rmse_train_sub3_pr, 5), \n",
    "      \"\\nValidation/Out-of-Sample: \", round(rmse_validate_sub3_pr, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c0bd1e",
   "metadata": {},
   "source": [
    "## The best model is Tweedie Regressor followed by OLS using the encoded bedbath_area and age_tax features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019045e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
